#### Overview

##### 组件

Spark应用在集群上以独立的进程集运行，由driver程序成的SparkContext协调。

SparkContext可以连接Spark自己的standalone cluster manager，Mesos，YARN，K8s等集群管理器。

连接上后，Spark会获取集群节点上的executor（executor是对你的应用进行计算和存储数据的进程）。然后Spark将应用代码发送到executor上（应用代码是由JAR或者Python文件定义并传递到SparkContext）。最后SparkContext发送task给executor运行。

<img src="部署/cluster-overview.png" alt="Spark cluster components" style="zoom:50%;" />

*注意：*

1. 一个应用有一个SparkContext。每个应用有属于自己的executor进程，在应用的整个生命周期内进程会一直运行，并且运行task在多个线程里。这是为了将来自调度端（每个driver调度自己的task）和executor端（来自不同应用的task运行在不同的JVM中）隔离。这样的话不同应用之间就不能共享数据，除非把数据写到外部存储系统中。
2. Spark不知道底层的集群管理器是什么，他只要能在上面获得executor进程并且进程间可以通信就行了。
3. driver程序的整个生命周期都要去监听并且接受来自它的executor上的连接。driver程序必须能够从worker节点进行网络寻址。
4. driver在集群上调度task，那么driver应该运行在worker节点附近，最好是相同的局域网内。如果想给远程的集群发送请求，最好在driver程序上开一个RPC，然后让他在worker节点附近提交操作，尽量不要在离worker节点太远。

##### 集群管理器类型

Standalone-Spark自带的、Hadoop YARN。(还有Mesos，K8s)

##### 提交应用

看应用提交指南(application submission guide)

##### 监控集群

看监控指南(monitoring guide)

##### Job调度

看Job调度概述(job scheduling overview)

##### 术语表

| Term            | Meaning                                                      |
| :-------------- | :----------------------------------------------------------- |
| Application     | 基于Spark构建的用户程序，由集群上的driver程序和executor组成  |
| Application jar | 包含用户Spark应用的jar包                                     |
| Driver program  | 一个进程，运行应用的main方法和创建SparkContext               |
| Cluster manager | 像YARN的资源调度器                                           |
| Deploy mode     | 告诉driver进程在哪运行，“cluster”是在集群里，“client”是在集群外 |
| Worker node     | 能在集群上运行应用代码的任一节点                             |
| Executor        | 在wordker节点上为应用启动的进程，它运行task，还可以持久化数据。每个应用都有它自己的executor |
| Task            | 一个会被发送到executor上的工作单元                           |
| Job             | 包含多个task的并行计算，task依据由Spark的action算子生成（比如save、collect） |
| Stage           | 每个Job会被分成多个更小的task集合，这些集合叫stages，stage间相互依赖 |

#### 提交应用

使用bin目录下的spark-submit脚本提交应用，对Spark支持的任何集群管理器都是适用，不用单独配置。

##### 构建你的应用依赖

如果你的应用依赖于其他项目，需要将它们打包到一起。带包的项目可以不用带Hadoop和Saprk依赖，因为集群已经提供了。

对于Python，添加第三方依赖可以：

只能上传.py, .zip, .egg文件

- 设置配置文件 spark.submit.pyFiles
- 脚本参数 --py-files
- 应用里直接调用pyspark.SparkContext.addPyFile()

##### 用spark-submit启动应用

```shell
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
```

--class:含有main方法的类

--master:集群的master URL（local[*]\local[2]\spark://hadoop101:7077\yarn）

--deploy-mode:cluster\client(default)

--conf:键值对类型key=value。多个就用--conf k1=v1 --conf k2=v2

*常用的就是从入口机器提交应用（与worker机器一块工作），这种最好是用clinet，driver直接在spark-submit进程中启动作为集群的client，若是用client模式，那么input和output都会显示在控制台上，适合包含REPL的应用*

*如果是从一个离worker远的机器上提交应用，通常用cluster模式来减少driver和executor之间的网络延迟。对python的应用，standalong模式不支持cluster模式*

*更多可用的参数可以通过命令  ./bin/spark-submit --help 查看*

##### Master URLs

| Master URL                        | Meaning                                                      |
| :-------------------------------- | :----------------------------------------------------------- |
| `local`                           | Run Spark locally with one worker thread (i.e. no parallelism at all). |
| `local[K]`                        | Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). |
| `local[K,F]`                      | Run Spark locally with K worker threads and F maxFailures (see [spark.task.maxFailures](https://spark.apache.org/docs/latest/configuration.html#scheduling) for an explanation of this variable). |
| `local[*]`                        | Run Spark locally with as many worker threads as logical cores on your machine. |
| `local[*,F]`                      | Run Spark locally with as many worker threads as logical cores on your machine and F maxFailures. |
| `local-cluster[N,C,M]`            | Local-cluster mode is only for unit tests. It emulates a distributed cluster in a single JVM with N number of workers, C cores per worker and M MiB of memory per worker. |
| `spark://HOST:PORT`               | Connect to the given [Spark standalone cluster](https://spark.apache.org/docs/latest/spark-standalone.html) master. The port must be whichever one your master is configured to use, which is 7077 by default. |
| `spark://HOST1:PORT1,HOST2:PORT2` | Connect to the given [Spark standalone cluster with standby masters with Zookeeper](https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper). The list must have all the master hosts in the high availability cluster set up with Zookeeper. The port must be whichever each master is configured to use, which is 7077 by default. |
| `mesos://HOST:PORT`               | Connect to the given [Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use `mesos://zk://...`. To submit with `--deploy-mode cluster`, the HOST:PORT should be configured to connect to the [MesosClusterDispatcher](https://spark.apache.org/docs/latest/running-on-mesos.html#cluster-mode). |
| `yarn`                            | Connect to a [YARN ](https://spark.apache.org/docs/latest/running-on-yarn.html)cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The cluster location will be found based on the `HADOOP_CONF_DIR` or `YARN_CONF_DIR` variable. |
| `k8s://HOST:PORT`                 | Connect to a [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) cluster in `client` or `cluster` mode depending on the value of `--deploy-mode`. The `HOST` and `PORT` refer to the [Kubernetes API Server](https://kubernetes.io/docs/reference/generated/kube-apiserver/). It connects using TLS by default. In order to force it to use an unsecured connection, you can use `k8s://http://HOST:PORT`. |

##### 加载配置文件

spark-submit默认情况下会加载配置文件./conf/spark-defaults.conf文件，将配置传递到应用上。

在配置文件中的属性，就不需要要再spark-submit上指定了，比如说在文件里写上spark.master，那就可以省略--master标签了。

配置文件的加载优先级为：1.SparkConf  2.spark-submit上的标签  3.配置文件

不知道配置来自什么地方，可用spark-submit --verbose查看更多debug细节。

#####  高级依赖管理

应用的jar和--jars上的jar都会被自动传送到集群，--jars后面的jar用逗号分隔开。

用下面几种方法可以传递jars：

- **file:** - Absolute paths and `file:/` URIs are served by the driver’s HTTP file server, and every executor pulls the file from the driver HTTP server.
- **hdfs:**, **http:**, **https:**, **ftp:** - these pull down files and JARs from the URI as expected
- **local:** - a URI starting with local:/ is expected to exist as a local file on each worker node. This means that no network IO will be incurred, and works well for large files/JARs that are pushed to each worker, or shared via NFS, GlusterFS, etc.

jar包和文件都会被复制到每个executor上SparkContext的工作目录上，随着时间的推移，这会显著增加空间的使用率，需要清理。如果是YARN模式，则是自动清理。Saprk standalong，自动清理需要设置spark.worker.cleanup.appDataTtl

**(不懂）Users may also include any other dependencies by supplying a comma-delimited list of Maven coordinates with `--packages`. All transitive dependencies will be handled when using this command. Additional repositories (or resolvers in SBT) can be added in a comma-delimited fashion with the flag `--repositories`. (Note that credentials for password-protected repositories can be supplied in some cases in the repository URI, such as in `https://user:password@host/...`. Be careful when supplying credentials this way.) These commands can be used with `pyspark`, `spark-shell`, and `spark-submit` to include Spark Packages.**
